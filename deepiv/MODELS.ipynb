{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import warnings\n",
    "\n",
    "# import deepiv.samplers as samplers\n",
    "# import deepiv.densities as densities\n",
    "# from deepiv.custom_gradients import replace_gradients_mse\n",
    "\n",
    "#---- From the modules ---\n",
    "import samplers as samplers\n",
    "import densities as densities\n",
    "from custom_gradients import replace_gradients_mse\n",
    "#-------------------------\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Lambda, InputLayer\n",
    "from keras.engine import topology\n",
    "try:\n",
    "    import h5py\n",
    "except ImportError:\n",
    "    h5py = None\n",
    "\n",
    "\n",
    "import keras.utils\n",
    "\n",
    "import numpy\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Treatment(Model):\n",
    "    '''\n",
    "    Adds sampling functionality to a Keras model and \n",
    "    extends the losses to support mixture of gaussian losses.\n",
    "\n",
    "    # Argument\n",
    "    '''\n",
    "\n",
    "    def _get_sampler_by_string(self, loss):\n",
    "        '''\n",
    "        loss is a string here\n",
    "        '''\n",
    "        output = self.outputs[0] # where to get output\n",
    "        inputs = self.inputs\n",
    "\n",
    "        if loss in [\"MSE\", \"mse\", \"mean_squared_error\"]:\n",
    "            #       # samplers function is from this directory\n",
    "            output += samplers.random_normal(K.shape(output), mean=0.0, std=1.0)\n",
    "            draw_sample = K.function(inputs + [K.learning_phase()], [output])\n",
    "\n",
    "            def sample_gaussian(inputs, use_dropout=False):\n",
    "                '''\n",
    "                Helper to draw samples from a gaussian distribution\n",
    "                '''\n",
    "                return draw_sample(inputs + [int(use_dropout)])[0]\n",
    "\n",
    "            return sample_gaussian # return a function here\n",
    "\n",
    "        elif loss == \"binary_crossentropy\":\n",
    "            output = K.random_binomial(K.shape(output), p=output)\n",
    "            draw_sample = K.function(inputs + [K.learning_phase()], [output])\n",
    "\n",
    "            def sample_binomial(inputs, use_dropout=False):\n",
    "                '''\n",
    "                Helper to draw samples from a binomial distribution\n",
    "                '''\n",
    "                return draw_sample(inputs + [int(use_dropout)])[0]\n",
    "\n",
    "            return sample_binomial\n",
    "\n",
    "        elif loss in [\"mean_absolute_error\", \"mae\", \"MAE\"]:\n",
    "            output += samplers.random_laplace(K.shape(output), mu=0.0, b=1.0)\n",
    "            draw_sample = K.function(inputs + [K.learning_phase()], [output])\n",
    "            def sample_laplace(inputs, use_dropout=False):\n",
    "                '''\n",
    "                Helper to draw samples from a Laplacian distribution\n",
    "                '''\n",
    "                return draw_sample(inputs + [int(use_dropout)])[0]\n",
    "\n",
    "            return sample_laplace\n",
    "\n",
    "        elif loss == \"mixture_of_gaussians\":\n",
    "            pi, mu, log_sig = densities.split_mixture_of_gaussians(output, self.n_components)\n",
    "            samples = samplers.random_gmm(pi, mu, K.exp(log_sig))\n",
    "            draw_sample = K.function(inputs + [K.learning_phase()], [samples])\n",
    "            \n",
    "            \n",
    "            return lambda inputs, use_dropout: draw_sample(inputs + [int(use_dropout)])[0]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unrecognised loss: %s.\\\n",
    "                                       Cannot build a generic sampler\" % loss)\n",
    "\n",
    "    def _prepare_sampler(self, loss):\n",
    "        '''\n",
    "        Build sampler\n",
    "        '''\n",
    "        if isinstance(loss, str):\n",
    "            self._sampler = self._get_sampler_by_string(loss)\n",
    "        else:\n",
    "            warnings.warn(\"You're using a custom loss function. Make sure you implement\\\n",
    "                           the model's sample() fuction yourself.\")\n",
    "\n",
    "    def compile(self, \n",
    "                optimizer, \n",
    "                loss, \n",
    "                metrics=None, loss_weights=None,\n",
    "                sample_weight_mode=None, n_components=None, **kwargs):\n",
    "        '''\n",
    "        Overrides the existing keras compile function to add a sampler building\n",
    "        step to the model compilation phase. Once compiled, one can draw samples\n",
    "        from the network using the sample() function and adds support for mixture\n",
    "        of gaussian loss.\n",
    "\n",
    "        '''\n",
    "        if loss == \"mixture_of_gaussians\":\n",
    "            if n_components is None:\n",
    "                raise Exception(\"When using mixture of gaussian loss you must\\\n",
    "                                 supply n_components argument\")\n",
    "            self.n_components = n_components\n",
    "            self._prepare_sampler(loss)\n",
    "            loss = lambda y_true, y_pred: densities.mixture_of_gaussian_loss(y_true,\n",
    "                                                                             y_pred,\n",
    "                                                                             n_components)\n",
    "\n",
    "            def predict_mean(x, batch_size=32, verbose=0):\n",
    "                '''\n",
    "                Helper to just predict the expected value of the mixture\n",
    "                of gaussian rather than the parameters for the distribution.\n",
    "                '''\n",
    "                y_hat = super(Treatment, self).predict(x, batch_size, verbose)\n",
    "                n_c = n_components\n",
    "                return (y_hat[:, 0:n_c] * y_hat[:, n_c:2*n_c]).sum(axis=1, keepdims=True)\n",
    "\n",
    "            self.predict_mean = predict_mean\n",
    "        else:\n",
    "            self._prepare_sampler(loss)\n",
    "\n",
    "        super(Treatment, self).compile(optimizer, loss, metrics=metrics, loss_weights=loss_weights,\n",
    "                                       sample_weight_mode=sample_weight_mode, **kwargs)\n",
    "\n",
    "    def sample(self, inputs, n_samples=1, use_dropout=False):\n",
    "        '''\n",
    "        Draw samples from the keras model.\n",
    "        '''\n",
    "        if hasattr(self, \"_sampler\"):\n",
    "            if not isinstance(inputs, list):\n",
    "                inputs = [inputs]\n",
    "            inputs = [i.repeat(n_samples, axis=0) for i in inputs]\n",
    "            return self._sampler(inputs, use_dropout)\n",
    "        else:\n",
    "            raise Exception(\"Compile model with loss before sampling\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Response(Model):\n",
    "    '''\n",
    "    Extends the Keras Model class to support sampling from the Treatment\n",
    "    model during training.\n",
    "    Overwrites the existing fit_generator function.\n",
    "\n",
    "    # Arguments\n",
    "    In addition to the standard model arguments, a Response object takes\n",
    "    a Treatment object as input so that it can sample from the fitted treatment\n",
    "    distriubtion during training.\n",
    "    '''\n",
    "    def __init__(self, treatment, **kwargs):\n",
    "        if isinstance(treatment, Treatment):\n",
    "            self.treatment = treatment\n",
    "        else:\n",
    "            raise TypeError(\"Expected a treatment model of type Treatment. \\\n",
    "                             Got a model of type %s. Remember to train your\\\n",
    "                             treatment model first.\" % type(treatment))\n",
    "        super(Response, self).__init__(**kwargs)\n",
    "\n",
    "    def compile(self, optimizer, loss, metrics=None, loss_weights=None, sample_weight_mode=None,\n",
    "                unbiased_gradient=False,n_samples=1, batch_size=None):\n",
    "        super(Response, self).compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights,\n",
    "                                      sample_weight_mode=sample_weight_mode)\n",
    "        self.unbiased_gradient = unbiased_gradient\n",
    "        if unbiased_gradient:\n",
    "            if loss in [\"MSE\", \"mse\", \"mean_squared_error\"]:\n",
    "                if batch_size is None:\n",
    "                    raise ValueError(\"Must supply a batch_size argument if using unbiased gradients. Currently batch_size is None.\")\n",
    "                replace_gradients_mse(self, optimizer, batch_size=batch_size, n_samples=n_samples)\n",
    "            else:\n",
    "                warnings.warn(\"Unbiased gradient only implemented for mean square error loss. It is unnecessary for\\\n",
    "                              logistic losses and currently not implemented for absolute error losses.\")\n",
    "            \n",
    "\n",
    "    def fit(self, x=None, y=None, batch_size=512, epochs=1, verbose=1, callbacks=None,\n",
    "            validation_data=None, class_weight=None, initial_epoch=0, samples_per_batch=None,\n",
    "            seed=None, observed_treatments=None):\n",
    "        '''\n",
    "        Trains the model by sampling from the fitted treament distribution.\n",
    "\n",
    "        # Arguments\n",
    "            x: list of numpy arrays. The first element should *always* be the instrument variables.\n",
    "            y: (numpy array). Target response variables.\n",
    "            The remainder of the arguments correspond to the Keras definitions.\n",
    "        '''\n",
    "        batch_size = numpy.minimum(y.shape[0], batch_size)\n",
    "        if seed is None:\n",
    "            seed = numpy.random.randint(0, 1e6)\n",
    "        if samples_per_batch is None:\n",
    "            if self.unbiased_gradient:\n",
    "                samples_per_batch = 2\n",
    "            else:\n",
    "                samples_per_batch = 1\n",
    "\n",
    "        if observed_treatments is None:\n",
    "            generator = SampledSequence(x[1:], x[0], y, batch_size, self.treatment.sample, samples_per_batch)\n",
    "        else:\n",
    "            generator = OnesidedUnbaised(x[1:], x[0], y, observed_treatments, batch_size,\n",
    "                                         self.treatment.sample, samples_per_batch)\n",
    "        \n",
    "        steps_per_epoch = y.shape[0]  // batch_size\n",
    "        super(Response, self).fit_generator(generator=generator,\n",
    "                                            steps_per_epoch=steps_per_epoch,\n",
    "                                            epochs=epochs, verbose=verbose,\n",
    "                                            callbacks=callbacks, validation_data=validation_data,\n",
    "                                            class_weight=class_weight, initial_epoch=initial_epoch)\n",
    "\n",
    "    def fit_generator(self, **kwargs):\n",
    "        '''\n",
    "        We use override fit_generator to support sampling from the treatment model during training.\n",
    "\n",
    "        If you need this functionality, you'll need to build a generator that samples from the\n",
    "        treatment and performs whatever transformations you're performing. Please submit a pull\n",
    "        request if you implement this.\n",
    "        '''\n",
    "        raise NotImplementedError(\"We use override fit_generator to support sampling from the\\\n",
    "                                   treatment model during training.\")\n",
    "\n",
    "    def expected_representation(self, x, z, n_samples=100, batch_size=None, seed=None):\n",
    "        inputs = [z, x]\n",
    "        if not hasattr(self, \"_E_representation\"):\n",
    "            if batch_size is None:\n",
    "                batch_size = inputs[0].shape[0]\n",
    "                steps = 1\n",
    "            else:\n",
    "                steps = inputs[0].shape[0] // batch_size\n",
    "\n",
    "            intermediate_layer_model = Model(inputs=self.inputs,\n",
    "                                             outputs=self.layers[-2].output)\n",
    "            \n",
    "            def pred(inputs, n_samples=100, seed=None):\n",
    "                features = inputs[1]\n",
    "\n",
    "                samples = self.treatment.sample(inputs, n_samples)\n",
    "                batch_features = [features.repeat(n_samples, axis=0)] + [samples]\n",
    "                representation = intermediate_layer_model.predict(batch_features)\n",
    "                return representation.reshape((inputs[0].shape[0], n_samples, -1)).mean(axis=1)\n",
    "            self._E_representation = pred\n",
    "            return self._E_representation(inputs, n_samples, seed)\n",
    "        else:\n",
    "            return self._E_representation(inputs, n_samples, seed)\n",
    "\n",
    "    def conditional_representation(self, x, p):\n",
    "        inputs = [x, p]\n",
    "        if not hasattr(self, \"_c_representation\"):          \n",
    "            intermediate_layer_model = Model(inputs=self.inputs,\n",
    "                                             outputs=self.layers[-2].output)\n",
    "\n",
    "            self._c_representation = intermediate_layer_model.predict\n",
    "            return self._c_representation(inputs)\n",
    "        else:\n",
    "            return self._c_representation(inputs)\n",
    "\n",
    "    def dropout_predict(self, x, z, n_samples=100):\n",
    "        if isinstance(x, list):\n",
    "            inputs = [z] + x\n",
    "        else:\n",
    "            inputs = [z, x]\n",
    "        if not hasattr(self, \"_dropout_predict\"):\n",
    "            \n",
    "            predict_with_dropout = K.function(self.inputs + [K.learning_phase()],\n",
    "                                              [self.layers[-1].output])\n",
    "\n",
    "            def pred(inputs, n_samples = 100):\n",
    "                # draw samples from the treatment network with dropout turned on\n",
    "                samples = self.treatment.sample(inputs, n_samples, use_dropout=True)\n",
    "                # prepare inputs for the response network\n",
    "                rep_inputs = [i.repeat(n_samples, axis=0) for i in inputs[1:]] + [samples]\n",
    "                # return outputs from the response network with dropout turned on (learning_phase=0)\n",
    "                return predict_with_dropout(rep_inputs + [1])[0]\n",
    "            self._dropout_predict = pred\n",
    "            return self._dropout_predict(inputs, n_samples)\n",
    "        else:\n",
    "            return self._dropout_predict(inputs, n_samples)\n",
    "\n",
    "    def credible_interval(self, x, z, n_samples=100, p=0.95):\n",
    "        '''\n",
    "        Return a credible interval of size p using dropout variational inference.\n",
    "        '''\n",
    "        if isinstance(x, list):\n",
    "            n = x[0].shape[0]\n",
    "        else:\n",
    "            n = x.shape[0]\n",
    "        alpha = (1-p) / 2.\n",
    "        samples = self.dropout_predict(x, z, n_samples).reshape((n, n_samples, -1))\n",
    "        upper = numpy.percentile(samples.copy(), 100*(p+alpha), axis=1)\n",
    "        lower = numpy.percentile(samples.copy(), 100*(alpha), axis=1)\n",
    "        return lower, upper\n",
    "\n",
    "    def _add_constant(self, X):\n",
    "        return numpy.concatenate((numpy.ones((X.shape[0], 1)), X), axis=1)\n",
    "    \n",
    "    def predict_confidence(self, x, p):\n",
    "        if hasattr(self, \"_predict_confidence\"):\n",
    "            return self._predict_confidence(x, p)\n",
    "        else:\n",
    "            raise Exception(\"Call fit_confidence_interval before running predict_confidence\")\n",
    "\n",
    "    \n",
    "    def fit_confidence_interval(self, x_lo, z_lo, p_lo, y_lo, n_samples=100, alpha=0.):\n",
    "        eta_bar = self.expected_representation(x=x_lo, z=z_lo, n_samples=n_samples)\n",
    "        pca = PCA(1-1e-16, svd_solver=\"full\", whiten=True)\n",
    "        pca.fit(eta_bar)\n",
    "\n",
    "        eta_bar = pca.transform(eta_bar)\n",
    "        eta_lo_prime = pca.transform(self.conditional_representation(x_lo, p_lo))\n",
    "        eta_lo = self._add_constant(eta_lo_prime)\n",
    "\n",
    "        ols1 = linear_model.Ridge(alpha=alpha, fit_intercept=True)\n",
    "        ols1.fit(eta_bar, eta_lo_prime)\n",
    "        hhat = ols1.predict(eta_bar)\n",
    "        ols2 = linear_model.Ridge(alpha=alpha, fit_intercept=False)\n",
    "        ols2.fit(self._add_constant(hhat), y_lo)\n",
    "\n",
    "        yhat = ols2.predict(eta_lo)\n",
    "        hhi = numpy.linalg.inv(numpy.dot(eta_lo.T, eta_lo))\n",
    "\n",
    "        heh = numpy.dot(eta_lo.T, numpy.square(y_lo - yhat) * eta_lo)\n",
    "        V = numpy.dot(numpy.dot(hhi, heh), hhi)\n",
    "\n",
    "        def pred(xx, pp):\n",
    "            H = self._add_constant(pca.transform(self.conditional_representation(xx,pp)))\n",
    "            sdhb = numpy.sqrt(numpy.diag(numpy.dot(numpy.dot(H, V), H.T)))\n",
    "            hb = ols2.predict(H).flatten()\n",
    "            return hb, sdhb\n",
    "        \n",
    "        self._predict_confidence = pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SampledSequence(keras.utils.Sequence):\n",
    "    def __init__(self, features, instruments, outputs, batch_size, sampler, n_samples=1, seed=None):\n",
    "        self.rng = numpy.random.RandomState(seed)\n",
    "        if not isinstance(features, list):\n",
    "            features = [features.copy()]\n",
    "        else:\n",
    "            features = [f.copy() for f in features]\n",
    "        self.features = features\n",
    "        self.instruments = instruments.copy()\n",
    "        self.outputs = outputs.copy()\n",
    "        if batch_size < self.instruments.shape[0]:\n",
    "            self.batch_size = batch_size\n",
    "        else:\n",
    "            self.batch_size = self.instruments.shape[0]\n",
    "        self.sampler = sampler\n",
    "        self.n_samples = n_samples\n",
    "        self.current_index = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def __len__(self):\n",
    "        if isinstance(self.outputs, list):\n",
    "            return self.outputs[0].shape[0] // self.batch_size\n",
    "        else:\n",
    "            return self.outputs.shape[0] // self.batch_size\n",
    "\n",
    "    def shuffle(self):\n",
    "        idx = self.rng.permutation(numpy.arange(self.instruments.shape[0]))\n",
    "        self.instruments = self.instruments[idx,:]\n",
    "        self.outputs = self.outputs[idx,:]\n",
    "        self.features = [f[idx,:] for f in self.features]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        instruments = [self.instruments[idx*self.batch_size:(idx+1)*self.batch_size, :]]\n",
    "        features = [inp[idx*self.batch_size:(idx+1)*self.batch_size, :] for inp in self.features]\n",
    "        sampler_input = instruments + features\n",
    "        samples = self.sampler(sampler_input, self.n_samples)\n",
    "        batch_features = [f[idx*self.batch_size:(idx+1)*self.batch_size].repeat(self.n_samples, axis=0) for f in self.features] + [samples]\n",
    "        batch_y = self.outputs[idx*self.batch_size:(idx+1)*self.batch_size].repeat(self.n_samples, axis=0)\n",
    "        if idx == (len(self) - 1):\n",
    "            self.shuffle()\n",
    "        return batch_features, batch_y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnesidedUnbaised(SampledSequence):\n",
    "    def __init__(self, features, instruments, outputs, treatments, batch_size, sampler, n_samples=1, seed=None):\n",
    "        self.rng = numpy.random.RandomState(seed)\n",
    "        if not isinstance(features, list):\n",
    "            features = [features.copy()]\n",
    "        else:\n",
    "            features = [f.copy() for f in features]\n",
    "        self.features = features\n",
    "        self.instruments = instruments.copy()\n",
    "        self.outputs = outputs.copy()\n",
    "        self.treatments = treatments.copy()\n",
    "        self.batch_size = batch_size\n",
    "        self.sampler = sampler\n",
    "        self.n_samples = n_samples\n",
    "        self.current_index = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        idx = self.rng.permutation(numpy.arange(self.instruments.shape[0]))\n",
    "        self.instruments = self.instruments[idx,:]\n",
    "        self.outputs = self.outputs[idx,:]\n",
    "        self.features = [f[idx,:] for f in self.features]\n",
    "        self.treatments = self.treatments[idx,:]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instruments = [self.instruments[idx*self.batch_size:(idx+1)*self.batch_size, :]]\n",
    "        features = [inp[idx*self.batch_size:(idx+1)*self.batch_size, :] for inp in self.features]\n",
    "        observed_treatments = self.treatments[idx*self.batch_size:(idx+1)*self.batch_size, :]\n",
    "        sampler_input = instruments + features\n",
    "        samples = self.sampler(sampler_input, self.n_samples // 2)\n",
    "        samples = numpy.concatenate([observed_treatments, samples], axis=0)\n",
    "        batch_features = [f[idx*self.batch_size:(idx+1)*self.batch_size].repeat(self.n_samples, axis=0) for f in self.features] + [samples]\n",
    "        batch_y = self.outputs[idx*self.batch_size:(idx+1)*self.batch_size].repeat(self.n_samples, axis=0)\n",
    "        if idx == (len(self) - 1):\n",
    "            self.shuffle()\n",
    "        return batch_features, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(filepath, model):\n",
    "    if h5py is None:\n",
    "        raise ImportError('`load_weights` requires h5py.')\n",
    "\n",
    "    with h5py.File(filepath, mode='r') as f:\n",
    "        # set weights\n",
    "        topology.load_weights_from_hdf5_group(f['model_weights'], model.layers)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
